Can you read this page? 
https://github.com/ellamd/portrait-data-engineer-test/tree/main. 
Work as a Data Engineer, provide the step by step to execute 
this project on AWS cloud, using the similar services to provide 
the expected functionalities. Use the few number of service as possible, 
and consider using only PostgreSQL as DB. The CSV must be stored into s3. 
After, It will be loaded into Postgress Tables, applying data quality and modelling
best praticces. 
Use Airflow and dbt also into the containers. The code will be developed 
locally in my machine, and must be uploaded and run into the cloud services. 
Create documentation, terraform scripts, upload scripts and cicd scripts. 
Use only one container service for developing the pipeline, like EC2 
(and use all the services deployed on it). 
I dont want to install anything in my machine. In aws, use only EC2 and S3.
The name of the project and its artifacts must be easily configurated in a conf 
file, and all the artifacts name will consider the name of the project and the 
name of the environment (dev, prod, test).


Configure a procces if I need to change a python code, 
to aplly this change and make it available on the environment.

First, create a folder structure to my projec, with all mkdir commands.

After create the Terraform scripts. Ensure that The configuration VPN, 
SEcurity Groups and vpc are aligned. DO it with less effort possible to configure 
manually.

Include the best praticces to create diferents environments 
if needed, like, dev prod, stc. Do it for all systems within ec2, 
including airflow, dbt and also postgres. And also allow that 
all projects artefacts code like airflow, dbt and postgres be 
created with the option to choose the environment when it needed.

create separeted instances of postgress, one for airflow, 
other for data wharehousing.

create a log file generated when the user_data.sh is executed by terraform.
Ensure that all access are provided to users when creating docker environment.

Next step, the upload_file_to_s3.py must read all the csvs files saved 
within the sample_datasets and save it in amazon s3 bucket previously created. 
The name and environment of the bucket was already defined: 
so, the subfolder of this initial file must be the raw subfolder.
Ensure that the next responses are aligned with the previous 
code artifacts already released. Create also AWS credentials 
inside the container, needed to execute the code. Do it in a
 way that the credentials are passed by terraform code.



----------//----------//----------//----------//----------


ssh -i ec2_key.pem ubuntu@54.81.91.140
git pull origin main

cd docker
docker-compose build --no-cache